import tensorflow as tf
# 텐서플로우를 임포트

x1=[73.,93.,89.,96.,73.]
x2=[80.,88.,91.,98.,66.]
x3=[75.,93.,90.,100.,70.]
Y=[152.,185.,180.,196.,142.]
# data and label

w1=tf.Variable(tf.random_normal([1]))
w2=tf.Variable(tf.random_normal([1]))
w3=tf.Variable(tf.random_normal([1]))
b=tf.Variable(tf.random_normal([1]))
# random weights

learning_rate=0.000001
# learning_rate는 작은값으로

for i in range(1000+1):
    # Gradient descent 1000+1회 수행
    
    with tf.GradientTape() as tape:
    # with문 안에 있는 변수들의 변화를 tape에 기록
    
        hypothesis=w1*x1+w2*x2+w3*x3+b
        # H(x1,x2,x3) = w1 x1 + w2 x2 + w3 x3 + b
        
        cost=tf.reduce_mean(tf.square(hypothesis-Y))
        # tf.reduce_mean() : 평균
        # tf.square() : 제곱
        # cost는 에러(가설과 실제 데이터의 차이) 제곱의 평균
        
    w1_grad,w2_grad,w3_grad,b_grad=tape.gradient(cost,[w1,w2,w3,b])
    # W에 대한 gradient(미분값)는 W_grad에, b에 대한 gradient는 b_grad에
    
    w1.assign_sub(learning_rate*w1_grad)
    w2.assign_sub(learning_rate*w2_grad)
    w3.assign_sub(learning_rate*w3_grad)
    b.assign_sub(learning_rate*b_grad)
    # 값 갱신
    
    if i%50==0:
        print("{:5}|{:12.4f}".format(i,cost.numpy()))
        # cost값의 변화를 50번에 한번씩 출력
